//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//
// Copyright (c) 2016-24, Lawrence Livermore National Security, LLC
// and RAJA project contributors. See the RAJA/LICENSE file for details.
//
// SPDX-License-Identifier: (BSD-3-Clause)
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~//

#ifndef __NESTED_LOOP_MULTI_LAMBDA_PARAM_IMPL_HPP__
#define __NESTED_LOOP_MULTI_LAMBDA_PARAM_IMPL_HPP__

#include "RAJA_test-abs.hpp"

template <typename EXEC_POL,
          bool USE_RESOURCE,
          typename SEGMENTS,
          typename PARAMS,
          typename WORKING_RES,
          typename... Args>
typename std::enable_if<USE_RESOURCE>::type call_kernel(SEGMENTS&&  segs,
                                                        PARAMS&&    params,
                                                        WORKING_RES work_res,
                                                        Args&&... args)
{
  RAJA::kernel_param_resource<EXEC_POL>(segs, params, work_res, args...);
}

template <typename EXEC_POL,
          bool USE_RESOURCE,
          typename SEGMENTS,
          typename PARAMS,
          typename WORKING_RES,
          typename... Args>
typename std::enable_if<!USE_RESOURCE>::type
call_kernel(SEGMENTS&& segs, PARAMS&& params, WORKING_RES, Args&&... args)
{
  RAJA::kernel_param<EXEC_POL>(segs, params, args...);
}

//
//
// Define list of nested loop types the MultiLambdaParam test supports.
//
//
using MultiLambdaParamSupportedLoopTypeList =
    camp::list<DEPTH_3, DEVICE_DEPTH_3>;

//
//
// Matrix-Matrix Multiplication test.
//
//
template <typename WORKING_RES, typename EXEC_POLICY, bool USE_RESOURCE>
void KernelNestedLoopTest()
{

  constexpr static int N   = 100;
  constexpr static int DIM = 2;

  camp::resources::Resource host_res{camp::resources::Host()};
  WORKING_RES               work_res{WORKING_RES::get_default()};

  // Allocate Tests Data
  double* work_arrA = work_res.template allocate<double>(N * N);
  double* work_arrB = work_res.template allocate<double>(N * N);
  double* work_arrC = work_res.template allocate<double>(N * N);

  double* test_arrA = host_res.allocate<double>(N * N);
  double* test_arrB = host_res.allocate<double>(N * N);
  double* test_arrC = host_res.allocate<double>(N * N);

  double* check_arrC = host_res.allocate<double>(N * N);

  // Initialize RAJA Views
  RAJA::View<double, RAJA::Layout<DIM>> test_viewA(test_arrA, N, N);
  RAJA::View<double, RAJA::Layout<DIM>> test_viewB(test_arrB, N, N);
  RAJA::View<double, RAJA::Layout<DIM>> test_viewC(test_arrC, N, N);

  RAJA::View<double, RAJA::Layout<DIM>> work_viewA(work_arrA, N, N);
  RAJA::View<double, RAJA::Layout<DIM>> work_viewB(work_arrB, N, N);
  RAJA::View<double, RAJA::Layout<DIM>> work_viewC(work_arrC, N, N);

  // Initialize Data
  for (int row = 0; row < N; ++row)
  {
    for (int col = 0; col < N; ++col)
    {
      test_viewA(row, col) = row;
      test_viewB(row, col) = col;
      test_viewB(row, col) = 0;
    }
  }

  work_res.memcpy(work_arrA, test_arrA,
                  sizeof(double) * RAJA::stripIndexType(N * N));
  work_res.memcpy(work_arrB, test_arrB,
                  sizeof(double) * RAJA::stripIndexType(N * N));
  work_res.memcpy(work_arrC, test_arrC,
                  sizeof(double) * RAJA::stripIndexType(N * N));

  // Calculate Test data
  for (int row = 0; row < N; ++row)
  {
    for (int col = 0; col < N; ++col)
    {

      double dot = 0.0;
      for (int k = 0; k < N; ++k)
      {
        dot += test_viewA(row, k) * test_viewB(k, col);
      }
      test_viewC(row, col) = dot;
    }
  }

  // Calculate Working data
  call_kernel<EXEC_POLICY, USE_RESOURCE>(
      RAJA::make_tuple(RAJA::RangeSegment{0, N}, RAJA::RangeSegment{0, N},
                       RAJA::RangeSegment{0, N}),

      RAJA::tuple<double>{0.0},

      // Resource
      work_res,

      // lambda 0
      [=] RAJA_HOST_DEVICE(double& dot) { dot = 0.0; },

      // lambda 1
      [=] RAJA_HOST_DEVICE(int col, int row, int k, double& dot)
      { dot += work_viewA(row, k) * work_viewB(k, col); },

      // lambda 2
      [=] RAJA_HOST_DEVICE(int col, int row, double& dot)
      { work_viewC(row, col) = dot; }

  );

  work_res.memcpy(check_arrC, work_arrC,
                  sizeof(double) * RAJA::stripIndexType(N * N));

  RAJA::forall<RAJA::seq_exec>(
      RAJA::RangeSegment{0, N * N}, [=](RAJA::Index_type i)
      { ASSERT_TRUE(RAJA::test_abs(test_arrC[i] - check_arrC[i]) < 10e-8); });

  work_res.deallocate(work_arrA);
  work_res.deallocate(work_arrB);
  work_res.deallocate(work_arrC);

  host_res.deallocate(test_arrA);
  host_res.deallocate(test_arrB);
  host_res.deallocate(test_arrC);

  host_res.deallocate(check_arrC);
}

//
//
// Defining the Kernel Loop structure for MultiLambdaParam Nested Loop Tests.
//
//
template <typename POLICY_TYPE, typename POLICY_DATA>
struct MultiLambdaParamNestedLoopExec;

template <typename POLICY_DATA>
struct MultiLambdaParamNestedLoopExec<DEPTH_3, POLICY_DATA>
{
  using type = RAJA::KernelPolicy<RAJA::statement::For<
      1,
      typename camp::at<POLICY_DATA, camp::num<0>>::type,
      RAJA::statement::For<
          0,
          typename camp::at<POLICY_DATA, camp::num<1>>::type,
          RAJA::statement::Lambda<0, RAJA::Params<0>>, // dot = 0.0
          RAJA::statement::For<
              2,
              typename camp::at<POLICY_DATA, camp::num<2>>::type,
              RAJA::statement::Lambda<1> // inner loop: dot += ...
              >,
          RAJA::statement::Lambda<2,
                                  RAJA::Segs<0, 1>,
                                  RAJA::Params<0>> // set
                                                   // C(row,
                                                   // col)
                                                   // = dot
          >>>;
};

#if defined(RAJA_ENABLE_CUDA) or defined(RAJA_ENABLE_HIP) or                   \
    defined(RAJA_ENABLE_SYCL)

template <typename POLICY_DATA>
struct MultiLambdaParamNestedLoopExec<DEVICE_DEPTH_3, POLICY_DATA>
{
  using type =
      RAJA::KernelPolicy<RAJA::statement::DEVICE_KERNEL<RAJA::statement::For<
          1,
          typename camp::at<POLICY_DATA, camp::num<0>>::type,
          RAJA::statement::For<
              0,
              typename camp::at<POLICY_DATA, camp::num<1>>::type,
              RAJA::statement::Lambda<0, RAJA::Params<0>>, // dot = 0.0
              RAJA::statement::For<
                  2,
                  typename camp::at<POLICY_DATA, camp::num<2>>::type,
                  RAJA::statement::Lambda<1> // inner loop: dot += ...
                  >,
              RAJA::statement::Lambda<2,
                                      RAJA::Segs<0, 1>,
                                      RAJA::Params<0>> // set C(row, col) = dot
              >>>                                      // end CudaKernel
                         >;
};

#endif // RAJA_ENABLE_CUDA or RAJA_ENABLE_HIP

#endif // __NESTED_LOOP_MULTI_LAMBDA_PARAM_IMPL_HPP__
