.. ##
.. ## Copyright (c) 2016, Lawrence Livermore National Security, LLC.
.. ##
.. ## Produced at the Lawrence Livermore National Laboratory.
.. ##
.. ## All rights reserved.
.. ##
.. ## For release details and restrictions, please see raja/README-license.txt
.. ##


===================================
Future Development
===================================

RAJA is very much a work-in-progress.  Additional features will appear in 
future releases. Items currently in development or planned include:

  * To complement RAJA, we are developing a resource manager runtime layer 
    that moves data automatically to desired memory locations based on 
    RAJA execution contexts. So, for example, applications using RAJA will
    have option to use either Unified Memory or explicit host-to-device and
    device-to-host data transfers when using CUDA. Similarly, for using other
    forms of high-bandwidth memory on other architectures. This can be done 
    in a manner that is larger transparent to application code. Cool, huh?

  * A cleaner implementation of IndexSet that will allow compile-time 
    selection of Segment types.  This will make it easier to add new
    Segment types by avoiding the need for switch statements in the 
    implementation. It will also help to reduce code bloat since only code
    for the Segment types needed will be generated by the compiler.

  * Other configuration improvements to make it easier for uses to select
    the sort of functionality they need at compile time; e.g., segment types,
    forall templates, etc.

  * Additional Segment and IndexSet support for nested-loops. We have some
    of this in the code now (forallN stuff), but there are other ways that 
    may be easier for different applications to use.

  * Support for CUDA streams, which can yield a significant performance
    boost on GPU systems.

  * Removal of virtual inheritance in the IndexSet Segment types. This 
    prevents Segment objects from being created in host code and then
    passing them as arguments to '__global__' CUDA functions to be used
    in GPU device code. This is the main reason why we do not have a 
    fully-functional GPU version of the CoMD proxy app.

  * CUDA versions of the nested loop (forallN) functionality. Currently, we
    are exploring ways to make it easier to manage the fact that CUDA 
    requires the '__device__' decoration on a C++ lambda when it will 
    execute in a device kernel. This makes it difficult to write generic 
    code that will run on either a CPU or GPU. So, rather than confuse
    folks, we left this out for now. 

  * A faster implementation of the CUDA reductions. We've been incrementally
    improving performance for a while and we think we can do better.

  * Additional tests and example codes.

**Stay tuned...**
