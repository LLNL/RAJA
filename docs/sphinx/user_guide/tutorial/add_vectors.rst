.. ##
.. ## Copyright (c) 2016-22, Lawrence Livermore National Security, LLC
.. ## and RAJA project contributors. See the RAJA/LICENSE file
.. ## for details.
.. ##
.. ## SPDX-License-Identifier: (BSD-3-Clause)
.. ##

.. _addvectors-label:

--------------------------------------
Vector Addition (Basic Loop Execution)
--------------------------------------

Key RAJA features shown in this example are:

  * ``RAJA::forall`` loop execution template
  * ``RAJA::RangeSegment`` iteration space construct
  * RAJA execution policies

The file ``RAJA/examples/tut_add-vectors.cpp`` contains complete working code
for the examples discussed below.

In the example, we add two vectors 'a' and 'b' of length N and
store the result in vector 'c'. A simple C-style loop that does this is:

.. literalinclude:: ../../../../examples/tut_add-vectors.cpp
   :start-after: _cstyle_vector_add_start
   :end-before: _cstyle_vector_add_end
   :language: C++ 

^^^^^^^^^^^^^^^^^^^^^
RAJA Variants
^^^^^^^^^^^^^^^^^^^^^

For the RAJA variants of the vector addition kernel, we replace the C-style 
for-loop with a call to the ``RAJA::forall`` loop execution template method.
The method takes an iteration space and the vector addition loop body as
a C++ lambda expression. We pass a ``RAJA::RangeSegment(0, N)`` object, which 
describes a contiguous sequence of integral values [0, N) for the iteration
space (for more information about RAJA loop indexing concepts, 
see :ref:`index-label`). The loop execution template method requires an 
execution policy template type that specifies how the loop is to run
(for more information about RAJA execution policies, see :ref:`policies-label`).

For the RAJA sequential variant, we use the ``RAJA::seq_exec`` execution
policy type:

.. literalinclude:: ../../../../examples/tut_add-vectors.cpp
   :start-after: _rajaseq_vector_add_start
   :end-before: _rajaseq_vector_add_end
   :language: C++ 

The RAJA sequential execution policy enforces strictly sequential execution; 
in particular, no SIMD vectorization instructions or other substantial 
optimizations will be generated by the compiler. To attempt to force the 
compiler to generate SIMD vector instructions, we would use the RAJA SIMD 
execution policy:: 

  RAJA::simd_exec

Alternatively, RAJA provides a *loop execution* policy::

  RAJA::loop_exec

which allows the compiler to generate optimizations based on when its internal
heuristics suggest that it is safe to do so and potentially 
beneficial for performance, but the optimizations are not forced.

To run the kernel with OpenMP multithreaded parallelism on a CPU, we use the
``RAJA::omp_parallel_for_exec`` execution policy:

.. literalinclude:: ../../../../examples/tut_add-vectors.cpp
   :start-after: _rajaomp_vector_add_start
   :end-before: _rajaomp_vector_add_end
   :language: C++ 

This will distribute the loop iterations across CPU threads and run the 
loop over threads in parallel. In particular, this is what you would get if
you wrote the kernel using OpenMP pragmas directly::

  #pragma omp parallel for
  for (int i = 0; i < N; ++i) {
    c[i] = a[i] + b[i];
  }

To run the kernel on a CUDA GPU device, we use the ``RAJA::cuda_exec``
policy:

.. literalinclude:: ../../../../examples/tut_add-vectors.cpp
   :start-after: _rajacuda_vector_add_start
   :end-before: _rajacuda_vector_add_end
   :language: C++ 

Note that the CUDA execution policy type requires a template argument 
``CUDA_BLOCK_SIZE``, which specifies the number of threads to run in each 
CUDA thread block launched to run the kernel.

For additional performance tuning options, the ``RAJA::cuda_exec_explicit`` 
policy is also provided, which allows a user to specify the number of thread 
blocks allocated per streaming multiprocessor (SM). Note that the third 
boolean argument expressing asynchronous execution can be omitted, and is 
``false`` by default (a similar defaulted argument is also supported for other
RAJA CUDA policies):

.. literalinclude:: ../../../../examples/tut_add-vectors.cpp
   :start-after: _rajacuda_explicit_vector_add_start
   :end-before: _rajacuda_explicit_vector_add_end
   :language: C++ 

Since the lambda defining the loop body will be passed to a device kernel, 
it must be decorated with the ``__device__`` attribute.
This can be done directly or by using the ``RAJA_DEVICE`` macro.

Similarly, to run the kernel on a GPU using the RAJA HIP back-end, 
we use the ``RAJA::hip_exec`` policy:

.. literalinclude:: ../../../../examples/tut_add-vectors.cpp
   :start-after: _rajahip_vector_add_start
   :end-before: _rajahip_vector_add_end
   :language: C++

